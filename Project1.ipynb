{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0XIKHQraRw9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import cv2 as cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5Syy5T-me7X"
      },
      "outputs": [],
      "source": [
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import glob\n",
        "from matplotlib import pyplot as plt\n",
        "list1=glob.glob('/content/drive/MyDrive/datatstes/fresh_banana/*.png')\n",
        "list2=glob.glob('/content/drive/MyDrive/datatstes/rotten_banana/*.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFlWaWAemlV-"
      },
      "outputs": [],
      "source": [
        "def labeling(name_list,label, X, y):\n",
        "    for i in name_list:\n",
        "        y.append(label)\n",
        "        img=cv.imread(i,1) \n",
        "        img=np.asarray(cv.resize(img, (96, 96)))\n",
        "        X.append(img)\n",
        "\n",
        "X=[]\n",
        "y=[]\n",
        "labeling(list1,np.array([1, 0]),X,y)\n",
        "labeling(list2,np.array([0, 1]),X,y)\n",
        "X = np.asarray(X)\n",
        "y = np.asarray(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oI-sbsNP-3lI",
        "outputId": "8cd65434-f115-4031-d290-3514337b26d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(885, 96, 96, 3) (885, 2)\n",
            "(222, 96, 96, 3) (222, 2)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "img_train, img_test, labels_train, labels_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
        "print(img_train.shape, labels_train.shape)\n",
        "print(img_test.shape, labels_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fFb_jDwc9DF",
        "outputId": "f168a116-72eb-451a-caf8-a7c38622f130"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2 class: ['dog', 'other']\n",
            "Image size: [96, 96, 3]\n"
          ]
        }
      ],
      "source": [
        "Path = '/content/drive/MyDrive/drive-download-20230323T162406Z-002/'\n",
        "\n",
        "class_names = sorted(os.listdir(Path))\n",
        "class_names.remove('labels.csv')\n",
        "num_classes = len(class_names)\n",
        "\n",
        "img_size = [96, 96, 3]\n",
        "print(f'{num_classes} class: {class_names}\\nImage size: {img_size}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "zCOx2r14dUAh",
        "outputId": "e042eb8d-3451-4d9d-e8dd-4faac2d38d46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label Data Frame:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3dff5d48-933e-48bd-9f80-c0c271d648cd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>filename</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>QYjQz1LS7dATFSYYRfnOfk6ezX8Gp49589.jpg</td>\n",
              "      <td>other</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>rltnrLmjyQgZnuSpbTKjPrm24wjHL43876.jpg</td>\n",
              "      <td>other</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>LNZNJPLSMhZAo8siLJlLVdHoxCTU5T11617.jpg</td>\n",
              "      <td>other</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>5ghe8eSnGWzfpowvy0oeLnqkEPqYbO8045.jpg</td>\n",
              "      <td>other</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2V3zbjyoxmG3zKDyxTpN2GpEvVolj811765.jpg</td>\n",
              "      <td>dog</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3dff5d48-933e-48bd-9f80-c0c271d648cd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3dff5d48-933e-48bd-9f80-c0c271d648cd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3dff5d48-933e-48bd-9f80-c0c271d648cd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Unnamed: 0                                 filename  label\n",
              "0           0   QYjQz1LS7dATFSYYRfnOfk6ezX8Gp49589.jpg  other\n",
              "1           1   rltnrLmjyQgZnuSpbTKjPrm24wjHL43876.jpg  other\n",
              "2           2  LNZNJPLSMhZAo8siLJlLVdHoxCTU5T11617.jpg  other\n",
              "3           3   5ghe8eSnGWzfpowvy0oeLnqkEPqYbO8045.jpg  other\n",
              "4           4  2V3zbjyoxmG3zKDyxTpN2GpEvVolj811765.jpg    dog"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "images = []\n",
        "labels = []\n",
        "labels_df = pd.read_csv(Path + 'labels.csv')\n",
        "\n",
        "print('Label Data Frame:')\n",
        "labels_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZ-4pC3ndbTD"
      },
      "outputs": [],
      "source": [
        "for image in labels_df.iloc:\n",
        "    try:\n",
        "      images.append(np.asarray(cv.resize(cv.imread(Path + image[2] + '/' + image[1], 1), img_size[0:2])[:, :, ::-1]))\n",
        "      label = np.zeros(num_classes)\n",
        "      label[class_names.index(image[2])] = 1\n",
        "      labels.append(label)\n",
        "    except:\n",
        "      continue\n",
        "\n",
        "images = np.asarray(images)\n",
        "labels = np.asarray(labels)\n",
        "\n",
        "print(f'\\nlabels shape: {labels.shape}')\n",
        "print(f'images shape: {images.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyy9fKPGi6i7",
        "outputId": "aa9bf7ff-bcb7-4147-aab2-4ada5975bbdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2425, 96, 96, 3) (2425, 2)\n",
            "(607, 96, 96, 3) (607, 2)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "img_train, img_test, labels_train, labels_test = train_test_split(images, labels, test_size = 0.2, random_state = 42)\n",
        "print(img_train.shape, labels_train.shape)\n",
        "print(img_test.shape, labels_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okss1XJHjyg7"
      },
      "outputs": [],
      "source": [
        "img_train_scaled = img_train / 255.0\n",
        "img_test_scaled = img_test / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNp2X5Vq94Su"
      },
      "outputs": [],
      "source": [
        "def Conv2D(img, kernel):\n",
        "  y, x = img.shape[0:2]\n",
        "  a, b = kernel.shape\n",
        "  \n",
        "  y = y - a + 1\n",
        "  x = x - b + 1\n",
        "  \n",
        "  new_img = np.zeros((y, x))\n",
        "  \n",
        "  for i in range(x):\n",
        "    for j in range(y):\n",
        "      new_img[j, i] = np.sum(img[j:j+a, i:i+b]*kernel)\n",
        "  return new_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucieinkQu13J"
      },
      "outputs": [],
      "source": [
        "def Deconv2D(img, kernel):\n",
        "    y, x = img.shape\n",
        "    a, b = kernel.shape\n",
        "    size_out = (a + y -1, b + x - 1)\n",
        "    new_img = np.zeros(size_out)\n",
        "    for i in range(y):\n",
        "        for j in range(x):\n",
        "            bit_matrix = kernel * img[i, j]\n",
        "            new_img[i:i+kernel.shape[0], j:j+kernel.shape[1]] = new_img[i:i+kernel.shape[0], j:j+kernel.shape[1]] + bit_matrix\n",
        "    return new_img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-W9Dwp6ZSEcm"
      },
      "outputs": [],
      "source": [
        "class Conv2DLayer:\n",
        "  def __init__(self, img_size_input, img_size_output):\n",
        "    kernel_size = np.array(img_size_input[0:2]) - np.array(img_size_output[0:2]) + 1\n",
        "    list_kernel = []\n",
        "    for j in range(img_size_output[2]):\n",
        "      kernel = []\n",
        "      for i in range(img_size_input[2]):\n",
        "        kernel.append(np.random.randn(kernel_size[0], kernel_size[1]))\n",
        "      kernel = np.asarray(kernel).transpose((1, 2, 0))\n",
        "      list_kernel.append(kernel)\n",
        "    self.kernel_size = kernel_size\n",
        "    list_kernel = np.asarray(list_kernel)\n",
        "    self.list_kernel = list_kernel\n",
        "    self.img_size_input = img_size_input\n",
        "    self.img_size_output = img_size_output\n",
        "    self.is_forward = False\n",
        "\n",
        "  def ReLU(self, z):\n",
        "    z[z < 0] = 0\n",
        "    return z\n",
        "\n",
        "  def convolution(self, img):\n",
        "    z = []\n",
        "    for j in self.list_kernel:\n",
        "        temp_img = np.zeros(self.img_size_output[0:2])\n",
        "        for i in range(self.img_size_input[2]):\n",
        "          temp_img = temp_img + Conv2D(img[:,:,i], j[:,:,i])\n",
        "        z.append(temp_img)\n",
        "    return np.asarray(z).transpose((1, 2, 0))\n",
        "\n",
        "  def forward(self, arr_img):\n",
        "    arr_img_conv = []\n",
        "    for img in arr_img:\n",
        "      arr_img_conv.append(self.convolution(img))\n",
        "    a = self.ReLU(np.asarray(arr_img_conv))\n",
        "    self.a = list(a)\n",
        "    self.x = arr_img\n",
        "    self.is_forward = True\n",
        "    return a\n",
        "\n",
        "  def back_propagation(self, x):\n",
        "    k = self.a\n",
        "    dkernel = []\n",
        "    for i in range(self.img_size_output[2]):\n",
        "      dkernel.append(np.zeros_like(self.list_kernel[i]))\n",
        "    dkernel = np.asarray(dkernel)\n",
        "    for t in range(self.x.shape[0]):\n",
        "      for i in range(self.img_size_output[2]):\n",
        "        for j in range(self.img_size_input[2]):\n",
        "            dkernel[i,:,:,j] = dkernel[i,:,:,j] + Conv2D(self.x[t,:,:,j], x[t,:,:,i])\n",
        "    dkernel = dkernel / (len(k) * dkernel.max())\n",
        "\n",
        "    dx = []\n",
        "    for a in k:\n",
        "      dx_C = np.zeros(self.img_size_input)\n",
        "      for t in range(self.img_size_output[2]):\n",
        "        for j in self.list_kernel:\n",
        "          for i in range(self.img_size_input[2]):\n",
        "            dx_C[:,:,i] = dx_C[:,:,i] + Deconv2D(a[:,:,t], j[:,:,i])\n",
        "      dx.append(dx_C)\n",
        "    dx = self.ReLU(np.asarray(dx))\n",
        "\n",
        "    return np.asarray(dkernel), dx / (len(self.list_kernel) * dx.max())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0agxp10s-LW8"
      },
      "outputs": [],
      "source": [
        "class MaxPooling2DLayer:\n",
        "    def __init__(self, pool_size=(2, 2), strides=None):\n",
        "        self.pool_size = pool_size\n",
        "        if strides is None:\n",
        "            self.strides = pool_size\n",
        "        else:\n",
        "            self.strides = strides\n",
        "        self.is_forward = False\n",
        "\n",
        "    def pooling(self, X, h_out, w_out, c, pool_height, pool_width, stride_height, stride_width):\n",
        "        out = []\n",
        "        for i in range(c):\n",
        "          out_temp = np.zeros((h_out, w_out))\n",
        "          for j in range(h_out):\n",
        "            for k in range(w_out):\n",
        "              out_temp[j, k] = X[j*stride_height:j*stride_height+pool_height, k*stride_width:k*stride_width+pool_width, i].max()\n",
        "          out.append(out_temp)\n",
        "        out = np.asarray(out)\n",
        "        self.is_forward = True\n",
        "        return out.transpose((1, 2, 0))\n",
        "\n",
        "    def forward(self, X):\n",
        "        n, h_in, w_in, c = X.shape\n",
        "        pool_height, pool_width = self.pool_size\n",
        "        stride_height, stride_width = self.strides\n",
        "        h_out = (h_in - pool_height) // stride_height + 1\n",
        "        w_out = (w_in - pool_width) // stride_width + 1\n",
        "        out = []\n",
        "        for i in range(n):\n",
        "          out.append(self.pooling(X[i], h_out, w_out, c, pool_height, pool_width, stride_height, stride_width))\n",
        "        out = np.asarray(out)\n",
        "        self.x = X\n",
        "        self.cache = [pool_height, pool_width, stride_height, stride_width]\n",
        "        self.is_forward = True\n",
        "        return out\n",
        "\n",
        "    def back_propagation(self,dout):\n",
        "        x = self.x\n",
        "        n, h_back, w_back, c= x.shape\n",
        "        pool_height, pool_width, stride_height, stride_width = self.cache\n",
        "        \n",
        "        dx = []\n",
        "        for r in range(n):\n",
        "          dx_C = []\n",
        "          for k in range(c):\n",
        "            dback = np.zeros((h_back, w_back))\n",
        "            for i in range(dout[r,:,:,k].shape[0]):\n",
        "              for j in range(dout[r,:,:,k].shape[1]):\n",
        "                dback[i*stride_height:i*stride_height+pool_height, j*stride_width:j*stride_width+pool_width][np.unravel_index(np.argmax(x[r, i*stride_height:i*stride_height+pool_height, j*stride_width:j*stride_width+pool_width, k]), x[r, i*stride_height:i*stride_height+pool_height, j*stride_width:j*stride_width+pool_width, k].shape)] = dout[r,i,j,k]\n",
        "            dx_C.append(dback)\n",
        "          dx_C = np.asarray(dx_C)\n",
        "          dx.append(dx_C)\n",
        "        dx = np.asarray(dx)\n",
        "        return dx.transpose((0, 2, 3, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rhlh1rf3WEyn"
      },
      "outputs": [],
      "source": [
        "class AvgPooling2DLayer(MaxPooling2DLayer):\n",
        "    def pooling(self, X, h_out, w_out, c, pool_height, pool_width, stride_height, stride_width):\n",
        "          out = []\n",
        "          for i in range(c):\n",
        "            out_temp = np.zeros((h_out, w_out))\n",
        "            for j in range(h_out):\n",
        "              for k in range(w_out):\n",
        "                out_temp[j, k] = X[j*stride_height:j*stride_height+pool_height, k*stride_width:k*stride_width+pool_width, i].mean()\n",
        "            out.append(out_temp)\n",
        "          out = np.asarray(out)\n",
        "          return out.transpose((1, 2, 0))\n",
        "\n",
        "    def back_propagation(self,dout):\n",
        "        x = self.x\n",
        "        n, h_back, w_back, c= x.shape\n",
        "        pool_height, pool_width, stride_height, stride_width = self.cache\n",
        "        \n",
        "        dx = []\n",
        "        for r in range(n):\n",
        "          dx_C = []\n",
        "          for k in range(c):\n",
        "            dback = np.zeros((h_back, w_back))\n",
        "            for i in range(dout[r,:,:,k].shape[0]):\n",
        "              for j in range(dout[r,:,:,k].shape[1]):\n",
        "                dback[i*stride_height:i*stride_height+pool_height, j*stride_width:j*stride_width+pool_width] = dout[r,i,j,k] / (pool_height * pool_width)\n",
        "            dx_C.append(dback)\n",
        "          dx_C = np.asarray(dx_C)\n",
        "          dx.append(dx_C)\n",
        "        dx = np.asarray(dx)\n",
        "        return dx.transpose((0, 2, 3, 1))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeaGAuqLGC81"
      },
      "outputs": [],
      "source": [
        "class FlattenLayer:\n",
        "  def __init__(self, input_shape):\n",
        "    self.input_shape = input_shape\n",
        "    self.is_forward = False\n",
        "\n",
        "  def forward(self, data):\n",
        "    out = []\n",
        "    n = data.shape[0]\n",
        "    self.n = n\n",
        "    for i in range(n):\n",
        "      out.append(data[i,:,:,:].transpose(2, 0, 1).ravel())\n",
        "    self.is_forward = True\n",
        "    return np.asarray(out / np.max(out))\n",
        "  \n",
        "  def back_propagation(self, dx):\n",
        "    dback = []\n",
        "    for i in range(self.n):\n",
        "      dback.append(dx[i].reshape(self.input_shape[2],self.input_shape[0],self.input_shape[1]).transpose(1, 2, 0))\n",
        "    return np.asarray(dback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHd5OuTCrKLe"
      },
      "outputs": [],
      "source": [
        "class SigmoidLayer:\n",
        "    def __init__(self, n_inp, n_out, bias=True):\n",
        "        if bias: \n",
        "            n_inp += 1\n",
        "            w = np.random.randn(n_inp, n_out)\n",
        "            self.w = w\n",
        "            self.bias = bias\n",
        "            self.is_forward = False\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def forward(self, X):\n",
        "        if self.bias:\n",
        "            X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "\n",
        "        z = np.dot(X, self.w)\n",
        "        a = self.sigmoid(z)\n",
        "\n",
        "        self.is_forward = True\n",
        "        self.x = X\n",
        "        self.a = a\n",
        "        return a\n",
        "\n",
        "    def back_propagation(self, delta):\n",
        "        a = self.a\n",
        "        dw = np.dot(self.x.T, delta * a * (1.0 - a))\n",
        "\n",
        "        if self.bias:\n",
        "            da = np.dot(delta * a * (1.0 - a), self.w.T)[:, 1:]\n",
        "        else:\n",
        "            da = np.dot(delta * a * (1.0 - a), self.w.T)\n",
        "\n",
        "        return dw, da"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0g9p2ksUrL93"
      },
      "outputs": [],
      "source": [
        "class SoftmaxLayer:\n",
        "    def __init__(self, n_inp, n_out, bias=True):\n",
        "        if bias: \n",
        "            n_inp += 1\n",
        "        w = np.random.randn(n_inp, n_out)\n",
        "        self.w = w\n",
        "        self.bias = bias\n",
        "        self.is_forward = False\n",
        "\n",
        "    def softmax(self, z):\n",
        "        return np.exp(z)/np.sum(np.exp(z), axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "        if self.bias:\n",
        "            X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "\n",
        "        z = np.dot(X, self.w)\n",
        "        a = self.softmax(z)\n",
        "\n",
        "        self.is_forward = True\n",
        "        self.x = X\n",
        "        self.a = a\n",
        "        return a\n",
        "    \n",
        "    def loss(self, y):\n",
        "        y_pred = self.a\n",
        "        return -np.mean(np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred)))\n",
        "\n",
        "    def back_propagation(self, delta):\n",
        "        a = self.a\n",
        "        dw = np.dot(self.x.T, delta * a * (1.0 - a))\n",
        "\n",
        "        if self.bias:\n",
        "            da = np.dot(delta * a * (1.0 - a), self.w.T)[:, 1:]\n",
        "        else:\n",
        "            da = np.dot(delta * a * (1.0 - a), self.w.T)\n",
        "\n",
        "        return dw, da"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4ZUWOhDrTX7"
      },
      "outputs": [],
      "source": [
        "block1 = [Conv2DLayer([96, 96, 3], [94, 94, 2]), \n",
        "          Conv2DLayer([94, 94, 2], [88, 88, 1]), \n",
        "          MaxPooling2DLayer()]\n",
        "block2 = [Conv2DLayer([44, 44, 1], [40, 40, 1]), \n",
        "          Conv2DLayer([40, 40, 1], [32, 32, 1]), \n",
        "          AvgPooling2DLayer(), \n",
        "          FlattenLayer([16, 16, 1])]\n",
        "block3 = [SigmoidLayer(16*16*1, 256, bias = True),\n",
        "          SigmoidLayer(256, 128),\n",
        "          SigmoidLayer(128, 64),\n",
        "          SigmoidLayer(64, 32),\n",
        "          SoftmaxLayer(32, 2)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXwjS-iijYPZ"
      },
      "outputs": [],
      "source": [
        "model = [block1, block2, block3]\n",
        "#luu lai ket qua tai day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlWpFON4uctG"
      },
      "outputs": [],
      "source": [
        "def predict(X, model):\n",
        "  for i in model:\n",
        "    for j in i:\n",
        "      X = j.forward(X)\n",
        "  return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUWYQdT28ifN"
      },
      "outputs": [],
      "source": [
        "def train_step(X, y, model, lr, alpha):\n",
        "    y_pred = predict(X, model)\n",
        "    delta = y_pred - y\n",
        "\n",
        "    for i in reversed(block3):\n",
        "        dw, delta = i.back_propagation(delta)\n",
        "        i.w -= lr * dw\n",
        "\n",
        "    for i in reversed(block2[-2:]):\n",
        "        delta = i.back_propagation(delta)\n",
        "    for i in reversed(block2[:-2]):\n",
        "        dkernel, delta = i.back_propagation(delta)\n",
        "        i.list_kernel -= lr * dkernel\n",
        "    for i in reversed(block1[-1:]):\n",
        "        delta = i.back_propagation(delta)\n",
        "    for i in reversed(block1[:-1]):\n",
        "        dkernel, delta = i.back_propagation(delta)\n",
        "        i.list_kernel -= lr * dkernel + alpha * i.list_kernel\n",
        "    \n",
        "    print(f'w: {model[-1][-1].w}, loss: {model[-1][-1].loss(y)}')\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtVSf2mr02jF"
      },
      "outputs": [],
      "source": [
        "def batch_generator(X, y, batch_size=32):\n",
        "    idx = np.arange(X.shape[0])\n",
        "    np.random.shuffle(idx)\n",
        "    n_batch = len(idx) // batch_size\n",
        "\n",
        "    for i in range(n_batch + 1):\n",
        "        i_start = i * batch_size\n",
        "        i_stop = min((i+1) * batch_size, len(idx))\n",
        "        idx_batch = idx[i_start: i_stop]\n",
        "        X_batch = X[idx_batch, :]\n",
        "        y_batch = y[idx_batch, :]\n",
        "        yield X_batch, y_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jt-K6B7T_f0e"
      },
      "outputs": [],
      "source": [
        "def train(X, y, model, lr, alpha, n_epochs):\n",
        "    for i in range(n_epochs):\n",
        "        print(f'epoch: {i}')\n",
        "        for Xb, yb in batch_generator(X, y):\n",
        "            train_step(Xb, yb, model, lr, alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "pD5K2AJ3GEFD",
        "outputId": "e537fb1a-9f6d-47a2-af6e-c491258716da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 0\n",
            "w: [[-2.52017412e-01  9.19822153e-01]\n",
            " [-1.13657130e+00 -2.35866416e-03]\n",
            " [ 1.01310874e-01  2.55511020e-01]\n",
            " [-9.69960671e-01 -1.90538444e+00]\n",
            " [-8.35729144e-01 -4.48679642e-01]\n",
            " [ 5.09296298e-01  2.21186743e-01]\n",
            " [ 2.06765969e+00  7.17304060e-01]\n",
            " [-1.68895247e-01 -3.53863873e-01]\n",
            " [-1.45553563e+00 -5.45759564e-01]\n",
            " [ 6.65536601e-01 -1.31651219e+00]\n",
            " [ 5.43585309e-01 -6.86272401e-01]\n",
            " [-1.20533718e+00 -8.42633521e-01]\n",
            " [-3.67735993e-01 -1.23514046e+00]\n",
            " [ 1.56058081e+00 -1.35369517e+00]\n",
            " [ 1.68120799e+00 -7.00717404e-01]\n",
            " [-1.07125943e+00  2.01122446e+00]\n",
            " [ 9.74121998e-02 -6.07880129e-01]\n",
            " [-7.35223899e-01 -2.59375097e-01]\n",
            " [ 8.06735463e-01  1.44820745e+00]\n",
            " [ 1.23272397e+00  5.57013193e-01]\n",
            " [ 1.48662910e+00 -1.21859324e-01]\n",
            " [ 1.02066908e+00  2.63739583e+00]\n",
            " [-8.78220654e-02 -4.24410974e-02]\n",
            " [-2.01378899e+00  1.95812327e+00]\n",
            " [ 2.36037645e-02  9.32446590e-01]\n",
            " [-5.16548780e-01  1.66760142e+00]\n",
            " [ 1.14117050e+00  8.53578800e-01]\n",
            " [ 1.34645511e+00 -6.69467792e-02]\n",
            " [-1.07741662e+00 -5.06928569e-01]\n",
            " [-2.75698925e-01  6.48973691e-01]\n",
            " [-1.11086367e+00 -1.03318428e-01]\n",
            " [ 7.24425070e-01 -1.02305172e+00]\n",
            " [-1.89357321e+00 -9.00509063e-01]], loss: 76.35548517266656\n",
            "w: [[-0.2571207   0.92492544]\n",
            " [-1.14167384  0.00274387]\n",
            " [ 0.10040517  0.25641672]\n",
            " [-0.97463161 -1.9007135 ]\n",
            " [-0.84083454 -0.44357425]\n",
            " [ 0.50418989  0.22629315]\n",
            " [ 2.06439057  0.72057318]\n",
            " [-0.16891571 -0.35384341]\n",
            " [-1.46058307 -0.54071213]\n",
            " [ 0.66313862 -1.31411421]\n",
            " [ 0.5386441  -0.6813312 ]\n",
            " [-1.20901128 -0.83895942]\n",
            " [-0.36935048 -1.23352597]\n",
            " [ 1.5551427  -1.34825706]\n",
            " [ 1.67885659 -0.69836601]\n",
            " [-1.07204317  2.0120082 ]\n",
            " [ 0.09723492 -0.60770285]\n",
            " [-0.73540896 -0.25919003]\n",
            " [ 0.80424459  1.45069832]\n",
            " [ 1.23212975  0.55760741]\n",
            " [ 1.4824255  -0.11765572]\n",
            " [ 1.01623368  2.64183123]\n",
            " [-0.08909149 -0.04117168]\n",
            " [-2.01370659  1.95804087]\n",
            " [ 0.02300697  0.93304339]\n",
            " [-0.51804049  1.66909312]\n",
            " [ 1.13607253  0.85867677]\n",
            " [ 1.34647053 -0.0669622 ]\n",
            " [-1.0825499  -0.50179529]\n",
            " [-0.27578175  0.64905651]\n",
            " [-1.11591335 -0.09826874]\n",
            " [ 0.72447398 -1.02310063]\n",
            " [-1.89382185 -0.90026043]], loss: 48.56535699144143\n",
            "w: [[-0.26978346  0.9375882 ]\n",
            " [-1.15433316  0.01540319]\n",
            " [ 0.09969039  0.2571315 ]\n",
            " [-0.98671076 -1.88863436]\n",
            " [-0.85349653 -0.43091226]\n",
            " [ 0.49145893  0.23902411]\n",
            " [ 2.05617025  0.7287935 ]\n",
            " [-0.16896121 -0.35379791]\n",
            " [-1.4734287  -0.52786649]\n",
            " [ 0.65846289 -1.30943849]\n",
            " [ 0.52769667 -0.67038376]\n",
            " [-1.2210189  -0.8269518 ]\n",
            " [-0.37176598 -1.23111047]\n",
            " [ 1.5419487  -1.33506306]\n",
            " [ 1.6706789  -0.69018831]\n",
            " [-1.07357762  2.01354265]\n",
            " [ 0.09688006 -0.60734799]\n",
            " [-0.7356235  -0.25897549]\n",
            " [ 0.79947278  1.45547013]\n",
            " [ 1.22422793  0.56550923]\n",
            " [ 1.47311232 -0.10834254]\n",
            " [ 1.00700907  2.65105585]\n",
            " [-0.08972645 -0.04053671]\n",
            " [-2.01372406  1.95805834]\n",
            " [ 0.0225432   0.93350716]\n",
            " [-0.52342859  1.67448123]\n",
            " [ 1.12340764  0.87134166]\n",
            " [ 1.34627245 -0.06676412]\n",
            " [-1.09498812 -0.48935707]\n",
            " [-0.27602256  0.64929733]\n",
            " [-1.12855847 -0.08562362]\n",
            " [ 0.72446788 -1.02309452]\n",
            " [-1.89654679 -0.89753549]], loss: 84.23302170692088\n",
            "w: [[-0.28473613  0.95254087]\n",
            " [-1.16928942  0.03035945]\n",
            " [ 0.0968172   0.26000469]\n",
            " [-1.00167915 -1.87366596]\n",
            " [-0.86847348 -0.41593531]\n",
            " [ 0.47639411  0.25408893]\n",
            " [ 2.04555209  0.73941166]\n",
            " [-0.16918856 -0.35357056]\n",
            " [-1.48791832 -0.51337688]\n",
            " [ 0.65316959 -1.30414518]\n",
            " [ 0.51313998 -0.65582708]\n",
            " [-1.23255445 -0.81541625]\n",
            " [-0.37581363 -1.22706282]\n",
            " [ 1.52697034 -1.32008471]\n",
            " [ 1.66255832 -0.68206773]\n",
            " [-1.07333417  2.01329919]\n",
            " [ 0.0965397  -0.60700763]\n",
            " [-0.73618734 -0.25841165]\n",
            " [ 0.79308225  1.46186066]\n",
            " [ 1.21864718  0.57108998]\n",
            " [ 1.46100395 -0.09623418]\n",
            " [ 0.992024    2.66604091]\n",
            " [-0.09089537 -0.03936779]\n",
            " [-2.01393365  1.95826793]\n",
            " [ 0.02184258  0.93420777]\n",
            " [-0.52954666  1.68059929]\n",
            " [ 1.10845738  0.88629192]\n",
            " [ 1.34629574 -0.06678741]\n",
            " [-1.10921085 -0.47513434]\n",
            " [-0.2760917   0.64936647]\n",
            " [-1.14347846 -0.07070363]\n",
            " [ 0.72449165 -1.0231183 ]\n",
            " [-1.89904172 -0.89504056]], loss: 65.65002015986309\n",
            "w: [[-0.28411733  0.95192207]\n",
            " [-1.16868003  0.02975006]\n",
            " [ 0.09632716  0.26049474]\n",
            " [-1.00155339 -1.87379172]\n",
            " [-0.8678622  -0.41654659]\n",
            " [ 0.47665371  0.25382933]\n",
            " [ 2.04274147  0.74222228]\n",
            " [-0.16913222 -0.3536269 ]\n",
            " [-1.48745746 -0.51383774]\n",
            " [ 0.65423429 -1.30520989]\n",
            " [ 0.51254687 -0.65523396]\n",
            " [-1.23377537 -0.81419533]\n",
            " [-0.37370289 -1.22917356]\n",
            " [ 1.52683776 -1.31995212]\n",
            " [ 1.66158651 -0.68109593]\n",
            " [-1.07106319  2.01102822]\n",
            " [ 0.09771177 -0.6081797 ]\n",
            " [-0.73575947 -0.25883953]\n",
            " [ 0.79241238  1.46253053]\n",
            " [ 1.21842076  0.5713164 ]\n",
            " [ 1.46220565 -0.09743588]\n",
            " [ 0.99273186  2.66533305]\n",
            " [-0.0910368  -0.03922636]\n",
            " [-2.01385834  1.95819263]\n",
            " [ 0.02350714  0.93254322]\n",
            " [-0.52905506  1.6801077 ]\n",
            " [ 1.10907637  0.88567292]\n",
            " [ 1.34646754 -0.06695921]\n",
            " [-1.10875774 -0.47558745]\n",
            " [-0.27609311  0.64936788]\n",
            " [-1.14335557 -0.07082652]\n",
            " [ 0.7245208  -1.02314745]\n",
            " [-1.89880861 -0.89527367]], loss: 74.86219241564692\n",
            "w: [[-0.27841335  0.94621809]\n",
            " [-1.16297884  0.02404887]\n",
            " [ 0.09590081  0.26092108]\n",
            " [-0.99644557 -1.87889954]\n",
            " [-0.86215965 -0.42224914]\n",
            " [ 0.48226533  0.24821771]\n",
            " [ 2.04527058  0.73969317]\n",
            " [-0.16913026 -0.35362887]\n",
            " [-1.48182431 -0.51947089]\n",
            " [ 0.65666457 -1.30764016]\n",
            " [ 0.51556857 -0.65825566]\n",
            " [-1.22931606 -0.81865464]\n",
            " [-0.37053589 -1.23234056]\n",
            " [ 1.53192814 -1.3250425 ]\n",
            " [ 1.66383273 -0.68334215]\n",
            " [-1.07015751  2.01012254]\n",
            " [ 0.09840209 -0.60887002]\n",
            " [-0.73544658 -0.25915242]\n",
            " [ 0.79563659  1.45930632]\n",
            " [ 1.2209593   0.56877787]\n",
            " [ 1.46597453 -0.10120475]\n",
            " [ 0.99568317  2.66238175]\n",
            " [-0.09048552 -0.03977764]\n",
            " [-2.01375909  1.95809337]\n",
            " [ 0.02444386  0.9316065 ]\n",
            " [-0.52648852  1.67754116]\n",
            " [ 1.11477881  0.87997049]\n",
            " [ 1.34679195 -0.06728362]\n",
            " [-1.103151   -0.48119419]\n",
            " [-0.27599765  0.64927241]\n",
            " [-1.13772791 -0.07645418]\n",
            " [ 0.72452467 -1.02315132]\n",
            " [-1.89844798 -0.8956343 ]], loss: 56.45048499641934\n",
            "w: [[-0.29405632  0.96186107]\n",
            " [-1.17861742  0.03968745]\n",
            " [ 0.09428358  0.26253831]\n",
            " [-1.0118478  -1.86349732]\n",
            " [-0.87780184 -0.40660695]\n",
            " [ 0.46662688  0.26385616]\n",
            " [ 2.03193042  0.75303333]\n",
            " [-0.16916304 -0.35359608]\n",
            " [-1.49747471 -0.50382048]\n",
            " [ 0.65117733 -1.30215292]\n",
            " [ 0.5008536  -0.64354069]\n",
            " [-1.24323146 -0.80473924]\n",
            " [-0.37215531 -1.23072114]\n",
            " [ 1.5159048  -1.30901916]\n",
            " [ 1.65227806 -0.67178748]\n",
            " [-1.07011306  2.01007809]\n",
            " [ 0.09827965 -0.60874758]\n",
            " [-0.73557048 -0.25902851]\n",
            " [ 0.78461053  1.47033238]\n",
            " [ 1.21244569  0.57729147]\n",
            " [ 1.45435236 -0.08958258]\n",
            " [ 0.98233555  2.67572936]\n",
            " [-0.09198425 -0.03827892]\n",
            " [-2.01385997  1.95819425]\n",
            " [ 0.02455359  0.93149676]\n",
            " [-0.53284643  1.68389906]\n",
            " [ 1.09914017  0.89560913]\n",
            " [ 1.34679067 -0.06728234]\n",
            " [-1.118427   -0.46591819]\n",
            " [-0.27625346  0.64952823]\n",
            " [-1.15334498 -0.06083712]\n",
            " [ 0.72451482 -1.02314147]\n",
            " [-1.90113    -0.89295228]], loss: 71.64851087460266\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-216-25765f685e0e>:13: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.asarray(out / np.max(out))\n",
            "<ipython-input-213-734e924f9849>:63: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.asarray(dkernel), dx / (len(self.list_kernel) * dx.max())\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "epoch: 1\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "epoch: 2\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n",
            "w: [[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]], loss: nan\n"
          ]
        }
      ],
      "source": [
        "train(img_train_scaled, labels_train, model, 0.01, 0.7, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TH_6unEfczNH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}